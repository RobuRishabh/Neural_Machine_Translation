{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMtQSb0A4i-e"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "1. Neural Machine Translation (NMT) is the task of using artificial neural network models for translation from one language to the other.\n",
    "2. The NMT model generally consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation.\n",
    "3. This problem can be thought as a prediction problem, where given a sequence of words in source language as input, task is to predict the output sequence of words in target language.\n",
    "4. The dataset comes from http://www.manythings.org/anki/, where you may find tab delimited bilingual sentence pairs in different files based on the source and target language of your choice.\n",
    "5. For this project, you need to use French - English language pairs just to evaluate the projects uniformly for all students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buPwfDQEcFBR"
   },
   "source": [
    "#Step-1: Download and clean the data\n",
    "1. Download the data as zip file and extract it to corresponding txt file. Read this txt file and prepare the list of pairs of language phrases.\n",
    "2. Now, we will nedd to clean these pairs. For cleaning the text, some of the operations for cleaning are:\n",
    "\n",
    "\n",
    "*   Remove the non printable charaters, if any\n",
    "*   Remove punctuations and non-alphabetic charaters\n",
    "* Convert to lowercase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('go', 'va ')\n",
      "('go', 'marche')\n",
      "('go', 'en route ')\n",
      "('go', 'bouge ')\n",
      "('hi', 'salut ')\n",
      "('hi', 'salut')\n",
      "('run', 'cours')\n",
      "('run', 'courez')\n",
      "('run', 'prenez vos jambes  vos cous ')\n",
      "('run', 'file ')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by : \n",
    "    - Removing non-printable characters\n",
    "    - Removing punctuations and non-alphabetic characters\n",
    "    - Converting to lowercase\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\x20-\\x7E]', '', text) # Remove non-printable characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuations and non-alphabetic characters\n",
    "    return text.lower()  # Convert to lowercase\n",
    "\n",
    "def extract_and_clean_pairs(file_path):\n",
    "    \"\"\"\n",
    "    Reads a text file and extracts pairs of language phrases, then cleans them.\n",
    "    Args : file_path (str) : Path to the text file.\n",
    "    Returns : list : A list of cleaned language phrase pairs.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split the line into parts (assuming tab-separated fields)\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) >= 2: # Ensure there are at least two fields\n",
    "                source, target = parts[:2] # Extract the first two fields as a pair\n",
    "                # Clean each part\n",
    "                source_cleaned = clean_text(source)\n",
    "                target_cleaned = clean_text(target)\n",
    "                pairs.append((source_cleaned, target_cleaned))\n",
    "    return pairs \n",
    "\n",
    "file_path = 'fra.txt'\n",
    "cleaned_pairs = extract_and_clean_pairs(file_path)\n",
    "\n",
    "# Display a sample of the cleaned pairs\n",
    "for pair in cleaned_pairs[:10]:\n",
    "    print(pair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7ONs3_mC4hF"
   },
   "source": [
    "#Step-2: Split and prepare the data for training the model\n",
    "1. After cleaning the data, next you need to split the data in train and test.\n",
    "2. Then, you need to create separate tokenizer for both source language and target language.\n",
    "3. After creating the tokenizer, you need to encode and pad the input (source language) and output(target language) sequences w.r.t. their individual tokenizers and maximum sequence lengths.\n",
    "4. Here, in this problem you will essentially be predicting the words in target language, therefore output seuences will need to be converted in one hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Source batch shape: torch.Size([32, 10])\n",
      "Target batch shape: torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class LanguageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for language pairs\n",
    "    Args : \n",
    "    - inputs (array) : Input sequences\n",
    "    - outputs (array) : Output sequences\n",
    "    \"\"\"\n",
    "    def __init__ (self, inputs, outputs):\n",
    "        self.inputs = torch.tensor(inputs, dtype = torch.long)\n",
    "        self.outputs = torch.tensor(outputs, dtype = torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "    \n",
    "def prepare_data(file_path, test_size = 0.2, max_source_len=10, max_target_len=10):\n",
    "    # Load and clean data\n",
    "    pairs = extract_and_clean_pairs(file_path)\n",
    "    # Split into training and testing sets\n",
    "    source_texts, target_texts = zip(*pairs)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(source_texts, target_texts, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Tokenize source and target texts\n",
    "    source_tokenizer = Tokenizer()\n",
    "    target_tokenizer = Tokenizer()\n",
    "    source_tokenizer.fit_on_texts(X_train)\n",
    "    target_tokenizer.fit_on_texts(y_train)\n",
    "\n",
    "    # Encode and pad sequences\n",
    "    source_train_seq = source_tokenizer.texts_to_sequences(X_train)\n",
    "    source_test_seq = source_tokenizer.texts_to_sequences(X_test)\n",
    "    target_train_seq = target_tokenizer.texts_to_sequences(y_train)\n",
    "    target_test_seq = target_tokenizer.texts_to_sequences(y_test)\n",
    "\n",
    "    source_train_pad = pad_sequences(source_train_seq, maxlen=max_source_len, padding = 'post')\n",
    "    source_test_pad = pad_sequences(source_test_seq, maxlen=max_source_len, padding = 'post')\n",
    "    target_train_pad = pad_sequences(target_train_seq, maxlen=max_source_len, padding = 'post')\n",
    "    target_test_pad = pad_sequences(target_test_seq, maxlen=max_source_len, padding = 'post')\n",
    "\n",
    "    # integer indices for targets\n",
    "    train_dataset = LanguageDataset(source_train_pad, target_train_pad)\n",
    "    test_dataset = LanguageDataset(source_test_pad, target_test_pad)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    return (\n",
    "        train_loader, test_loader,\n",
    "        source_tokenizer, target_tokenizer, \n",
    "        max_source_len, max_target_len,\n",
    "        len(target_tokenizer.word_index) + 1  # Vocabulary size\n",
    "    )\n",
    "\n",
    "\n",
    "file_path = 'fra.txt'\n",
    "\n",
    "# Prepare data\n",
    "train_loader, test_loader, source_tokenizer, target_tokenizer, max_source_len, max_target_len, target_vocab_size = prepare_data(\n",
    "    file_path\n",
    ")\n",
    "\n",
    "# Example usage: iterate over the dataloader\n",
    "for batch in train_loader:\n",
    "    source_batch, target_batch = batch\n",
    "    print(\"Source batch shape:\", source_batch.shape)\n",
    "    print(\"Target batch shape:\", target_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COi2bEcXErqU"
   },
   "source": [
    "#Step-3: Define and train the RNN based Encoder-Decoder model\n",
    "1. First, you need to define the sequential model consisting mainly of two parts Encoder and Decoder \n",
    "2. In Encoder, the input sequence shall be passed through an Embedding layer (to train the word embeddings for source language) and then the output from the Embedding layer may be passed through one or more RNN/LSTM layers.\n",
    "3. Now, to connect this Encoder to Decoder (yet to be defined), we can use RepeatVector layer. (This is because the shape of the output by Encoder is not same as expected shape of Input by Decoder)\n",
    "4. Now, stack up the Decoder, wherein you may add one or more RNN/LSTM layers and finally the output TimeDistributed Dense layer to get output separately by timesteps.\n",
    "5. Now, you have defined the model and now this can be trained on the training data, you prepared in last step. Here, you may play with the number of epochs, optimizer, batch size to get the optimum results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers=1, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src) # [batch_size, src_len, emb_dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded) # outputs ignored; only need hidden, cell\n",
    "        return hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers=1, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # Final Dense layer for timestep-specific outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, hidden, cell):\n",
    "        trg = trg.unsqueeze(1)  # Add a time dimension: [batch_size] -> [batch_size, 1]\n",
    "        embedded = self.dropout(self.embedding(trg))  # [batch_size, 1, emb_dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))  # [batch_size, 1, hidden_dim]\n",
    "        prediction = self.fc(output.squeeze(1))  # [batch_size, output_dim]\n",
    "        return prediction, hidden, cell\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # First input to the decoder is the <sos> token\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            # Decide if we will use teacher forcing or not\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = trg[:, t] if teacher_force else output.argmax(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "INPUT_DIM = len(source_tokenizer.word_index) + 1 # Source vocabulary size\n",
    "OUTPUT_DIM = target_vocab_size  # Target vocabulary size\n",
    "ENC_EMB_DIM = 128  # Encoder embedding size\n",
    "DEC_EMB_DIM = 128  # Decoder embedding size\n",
    "HIDDEN_DIM = 256  # LSTM hidden size\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.2\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device being used: {DEVICE}\")\n",
    "\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 4.9880\n",
      "Epoch: 2, Train Loss: 3.2446\n",
      "Epoch: 3, Train Loss: 2.5833\n",
      "Epoch: 4, Train Loss: 2.2002\n",
      "Epoch: 5, Train Loss: 1.9389\n",
      "Epoch: 6, Train Loss: 1.7519\n",
      "Epoch: 7, Train Loss: 1.6119\n",
      "Epoch: 8, Train Loss: 1.5031\n",
      "Epoch: 9, Train Loss: 1.4110\n",
      "Epoch: 10, Train Loss: 1.3389\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, trg in iterator:\n",
    "        src, trg = src.to(DEVICE).long(), trg.to(DEVICE).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)  # [batch_size, trg_len, trg_vocab_size]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)  # Exclude <sos> token\n",
    "        trg = trg[:, 1:].reshape(-1)  # Exclude <sos> token\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Training loop\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upGUYAzRIYad"
   },
   "source": [
    "#Step-4: Evaluating the model\n",
    "Use BLEU score for evaluating your model using NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score on Test Set: 0.0970\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def evaluate_model(model, data_loader, source_tokenizer, target_tokenizer, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model using BLEU score on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Seq2Seq model.\n",
    "        data_loader: DataLoader for the test set.\n",
    "        source_tokenizer: Tokenizer for source language.\n",
    "        target_tokenizer: Tokenizer for target language.\n",
    "        device: Device (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        float: Average BLEU score for the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    bleu_scores = []\n",
    "    sos_token = 1  # Assuming <sos> is tokenized as 1\n",
    "    eos_token = 2  # Assuming <eos> is tokenized as 2\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg in data_loader:\n",
    "            src = src.to(device)\n",
    "\n",
    "            # Get the model predictions\n",
    "            batch_size = src.shape[0]\n",
    "            trg_len = trg.shape[1]\n",
    "            outputs = torch.zeros(batch_size, trg_len).to(device, dtype=torch.long)\n",
    "\n",
    "            hidden, cell = model.encoder(src)\n",
    "            input_token = torch.full((batch_size,), sos_token, dtype=torch.long).to(device)\n",
    "\n",
    "            for t in range(1, trg_len):\n",
    "                output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
    "                top1 = output.argmax(1)\n",
    "                outputs[:, t] = top1\n",
    "                input_token = top1\n",
    "\n",
    "            # Convert outputs to text\n",
    "            for i in range(batch_size):\n",
    "                predicted_seq = outputs[i].tolist()\n",
    "                target_seq = trg[i].tolist()\n",
    "\n",
    "                # Remove padding and special tokens\n",
    "                predicted_text = [\n",
    "                    target_tokenizer.index_word[token]\n",
    "                    for token in predicted_seq\n",
    "                    if token not in {0, sos_token, eos_token}\n",
    "                ]\n",
    "                target_text = [\n",
    "                    target_tokenizer.index_word[token]\n",
    "                    for token in target_seq\n",
    "                    if token not in {0, sos_token, eos_token}\n",
    "                ]\n",
    "\n",
    "                # Calculate BLEU score for the sequence\n",
    "                if target_text:  # Ensure target is not empty\n",
    "                    bleu_score = sentence_bleu(\n",
    "                        [target_text],\n",
    "                        predicted_text,\n",
    "                        smoothing_function=SmoothingFunction().method1,\n",
    "                    )\n",
    "                    bleu_scores.append(bleu_score)\n",
    "\n",
    "    return sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "test_bleu_score = evaluate_model(\n",
    "    model, test_loader, source_tokenizer, target_tokenizer, DEVICE\n",
    ")\n",
    "print(f\"Average BLEU Score on Test Set: {test_bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "neural_machine_translation_notes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
